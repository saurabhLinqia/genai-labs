{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with Llama 2 and Pgvector\n",
    "#### using Sagemaker Jumpstart Foundation models and RDS w/ Pgvector\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we'll use the previously created and populated vector store in RDS Postgres as well as the previously deployed Llama 2 endpoint to demonstrate Retrieval Augmented Generation for Q&A on the SEC document data embedded in the previous lab. \n",
    "\n",
    "---\n",
    "##### Lab Agenda:\n",
    "1. Setup dependencies\n",
    "2. Connect to the vector store\n",
    "3. Connect to the Llama 2 endpoint and test model interaction\n",
    "4. Setup the Text Embedding model embed the questions\n",
    "5. Bring it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. Setup dependencies\n",
    "check python version and import envrionment settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -V #should be 3.10.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install --upgrade pip --quiet\n",
    "!pip install --upgrade psycopg2-binary # python 3.8\n",
    "!pip install --upgrade pgvector --quiet\n",
    "!pip install --upgrade tiktoken --quiet\n",
    "!pip install --upgrade langchain --quiet\n",
    "!pip install --upgrade sagemaker --quiet\n",
    "!pip install --upgrade beautifulsoup4 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, json\n",
    "sys.path.append(\"libs\")\n",
    "\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from sagemaker_embeddings_model import SagemakerEmbeddingsModel\n",
    "# from sagemaker_textgen_model import SagemakerLlama2TextGenModel\n",
    "from pgclient import PgClient\n",
    "# from doc_to_vex import DocToVex\n",
    "import sagemaker_utils\n",
    "\n",
    "\n",
    "# TIKTOKEN_ENCODING = 'p50k_base'\n",
    "TEXT_EMBEDDING_ENDPOINT = \"### SET TEXT EMBEDDING ENDPOINT ###\"\n",
    "TEXT_GENERATION_ENDPOINT = \"### SET TEXT GENERATION ENDPOINT ###\"\n",
    "# EMBEDDING_VECTOR_SIZE = 384 # this depends on the model used to do the embeddings\n",
    "# RDS_CRED_SECRET_ID = \"genai/rds/pgvector-pub1\"\n",
    "REGION = \"us-east-1\"\n",
    "DB_SETTINGS_FILE = \"dbsettings.json\"\n",
    "TABLE_NAME = \"embeddings\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. Connect to the vector store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open( DB_SETTINGS_FILE, 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "    dbsettings = json.loads(content)\n",
    "\n",
    "# Setup the database client instance and connect\n",
    "db = PgClient(dbsettings)\n",
    "db.connect() # hardcoded to 'postgres' database for this demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the connection and check for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rez = db.query(f\"select id from {TABLE_NAME}\")\n",
    "print(\"records: \", len(rez))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. Connect to the Llama 2 endpoint and test model interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already deployed the model endpoints. let's veryify by listing the currently deployed sagemaker endpoints and get the full name of the Llama 2 endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_utils.list_endpoints()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Set our endpoint vars and setup the llama2 predictor class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_EMBEDDING_ENDPOINT = \"### SET TEXT EMBEDDING ENDPOINT ###\"\n",
    "TEXT_GENERATION_ENDPOINT = \"### SET TEXT GENERATION ENDPOINT ###\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the sagemaker session \n",
    "session = Session()\n",
    "\n",
    "# instantiate the predictor\n",
    "llama2 = Predictor(endpoint_name = TEXT_GENERATION_ENDPOINT, sagemaker_session = session)\n",
    "\n",
    "# define the query endpoint\n",
    "def query_endpoint(instruction, context_prompt, context, question, max_new_tokens=512):\n",
    "    # query the model\n",
    "    model_params = {\n",
    "        \"max_new_tokens\": max_new_tokens, \n",
    "        \"top_p\": 0.9, \n",
    "        \"temperature\": 0.6,\n",
    "        # \"return_full_text\": False,\n",
    "    }\n",
    "\n",
    "    prompt_array = []\n",
    "    \n",
    "    if instruction is not None:\n",
    "        prompt_array.append({\"role\": \"system\", \"content\": instruction })\n",
    "        \n",
    "    if context_prompt is not None:\n",
    "        prompt_array.append({\"role\": \"user\", \"content\": context_prompt })\n",
    "        \n",
    "    if context is not None:\n",
    "        prompt_array.append({\"role\": \"assistant\", \"content\": context })\n",
    "        \n",
    "    if question is not None: \n",
    "        prompt_array.append({\"role\": \"user\", \"content\": question })\n",
    "\n",
    "    prompt = [prompt_array]\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": model_params\n",
    "    }\n",
    "    \n",
    "    # print(\"encoding payload\")\n",
    "    encoded_payload = json.dumps(payload).encode(\"utf-8\")\n",
    "\n",
    "    query_response = llama2.predict( \n",
    "        encoded_payload,\n",
    "        {\n",
    "            \"ContentType\": \"application/json\",\n",
    "            \"Accept\": \"application/json\",\n",
    "            \"CustomAttributes\": \"accept_eula=true\" # for Meta models\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return json.loads(query_response)[0]['generation']['content']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model with some basic prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    " \n",
    "instruction = \"Answer in the form of a Haiku\" \n",
    "# instruction = \"Answer in the form of a poem\" \n",
    "# instruction = \"You are a travel advisor assistant\" \n",
    "# instruction = \"You are a high school science teacher\"\n",
    "\n",
    "context_prompt = None\n",
    "\n",
    "context = None\n",
    "\n",
    "question = \"What is the best pizza in New York City\"\n",
    "# question = \"What is the difference between nuclear fusion and nuclear fission\"\n",
    "\n",
    "result = query_endpoint(instruction, context_prompt, context, question)\n",
    "# generated = json.loads(result)\n",
    "\n",
    "print(\"generated text:\\n\", result, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4. Connect to the Text Embedding model to do vector queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instance the embedder with the previously deployed text embedding endpoint \n",
    "embedder = SagemakerEmbeddingsModel(TEXT_EMBEDDING_ENDPOINT, session)\n",
    "\n",
    "# test it \n",
    "vec = embedder.query_endpoint(\"Did it work?\")\n",
    "print(len(vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5. Bring it all together \n",
    "\n",
    "Now that we have an embedder to vectorize our questions and the Llama 2 endpoint to generate nice responses, we'll add our similarity query responses from the vector store to the LLM prompt to answer questions about the document \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some helper functions\n",
    "\n",
    "def similarity_search(question):\n",
    "    print(\"embed the question\")\n",
    "          \n",
    "    question_embedding = embedder.query_endpoint(question)\n",
    "    \n",
    "    vec_results = 3\n",
    "\n",
    "    print(\"query the vector store\")\n",
    "    # <-> l2 distance\n",
    "    # <=> cosine distance\n",
    "    # <#> inner product\n",
    "    # Note: <#> returns the negative inner product since Postgres only supports ASC order index scans on operators\n",
    "    query = \"\"\"SELECT id, source, content, \n",
    "                descriptions_embeddings <-> '{}' as distance\n",
    "                FROM embeddings \n",
    "                ORDER BY descriptions_embeddings <-> '{}' limit {};\"\"\".format(question_embedding, question_embedding, vec_results)\n",
    "\n",
    "    rez = db.query(query)\n",
    "    return rez\n",
    "\n",
    "\n",
    "def build_context_string(query_result):\n",
    "    print(\"parse the results and prompt the LLM\")\n",
    "    context = \"\"\n",
    "    for r in query_result:\n",
    "        context += r[2] + \"\\n\"\n",
    "    # print(context)\n",
    "    return context\n",
    "\n",
    "\n",
    "def rag_query(question, context):\n",
    "    \n",
    "    instruction = \"You are a helpful assistant that is good at giving succint answers to questions. If you don't know an answer, say Don't Know\"\n",
    "    context_prompt = \"What are some excerpts from a Company's annual meeting?\"\n",
    "    \n",
    "    return query_endpoint(instruction, context_prompt, context, question)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End to end RAG querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who is the Chief Executive Officer?\"\n",
    "# question = \"Who are the Board of Directors?\"\n",
    "# question = \"How many directors does the company have?\"\n",
    "# question = \"How much do the board members make?\"\n",
    "# question = \"What is the max number of directors?\"\n",
    "\n",
    "\n",
    "search_results = similarity_search(question)\n",
    "\n",
    "context = build_context_string(search_results)\n",
    "\n",
    "result = rag_query(question, context)\n",
    "\n",
    "print(\"\\nquestion: \\n\", question, \"\\n\")\n",
    "print(\"\\ngenerated text: \\n\", result, \"\\n\")\n",
    "print(\"\\nsource: \\n\", context)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
